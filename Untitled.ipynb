{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nlp():\n",
    "    filename='glove.6B.100d.txt.word2vec'\n",
    "    model=KeyedVectors.load_word2vec_format(filename,binary=False)\n",
    "    model.save('gensim.bin')\n",
    "    return model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rajath/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH='text_data/S08_set1_a'\n",
    "PATH2='.txt.clean'\n",
    "file_range=range(1,10)\n",
    "labels=[]\n",
    "txt_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_data/S08_set1_a1.txt.clean\n",
      "text_data/S08_set1_a2.txt.clean\n",
      "text_data/S08_set1_a3.txt.clean\n",
      "text_data/S08_set1_a4.txt.clean\n",
      "text_data/S08_set1_a5.txt.clean\n",
      "text_data/S08_set1_a6.txt.clean\n",
      "text_data/S08_set1_a7.txt.clean\n",
      "text_data/S08_set1_a8.txt.clean\n",
      "text_data/S08_set1_a9.txt.clean\n"
     ]
    }
   ],
   "source": [
    "for i in file_range:\n",
    "    File_Path=PATH+str(i)+PATH2\n",
    "    print(File_Path)\n",
    "    file=open(File_Path,'r')\n",
    "    txt=file.readlines()\n",
    "    topic=txt[0]\n",
    "    txt=txt[1:]\n",
    "    txt_data.append(txt)\n",
    "    labels.append(topic[:-1])    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(txt_data):\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data_list=[]\n",
    "    for data in txt_data:\n",
    "        paragraph_list=[]\n",
    "        for paragraph in data:\n",
    "            word_tokens=tokenizer.tokenize(paragraph)\n",
    "            filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "            filtered_sentence = [w.lower() for w in filtered_sentence if w]\n",
    "            #print(filtered_sentence)\n",
    "            paragraph_list.append(filtered_sentence)\n",
    "        data_list.append(paragraph_list)\n",
    "    return data_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(txt_list):\n",
    "    clr_text_data=[]\n",
    "    for topic in txt_list:\n",
    "        topic_list=[]\n",
    "        for item in topic:\n",
    "            if item:\n",
    "                topic_list.append(item)\n",
    "        clr_text_data.append(topic_list)\n",
    "    return clr_text_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data(clr_text_data):\n",
    "    data=[]\n",
    "    for topic in clr_text_data:\n",
    "        z=[x for sublist in topic for x in sublist]\n",
    "        data.append(z)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_txt_data=remove_stopwords(txt_data)\n",
    "clr_text_data=remove_empty(fil_txt_data)\n",
    "data=organize_data(clr_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'macropodidae' not in vocabulary\"\n",
      "\"word 'antilopine' not in vocabulary\"\n",
      "\"word 'wallaroos' not in vocabulary\"\n",
      "\"word 'pademelons' not in vocabulary\"\n",
      "\"word 'yimidhirr' not in vocabulary\"\n",
      "\"word 'gangurru' not in vocabulary\"\n",
      "\"word 'kangooroo' not in vocabulary\"\n",
      "\"word 'kanguru' not in vocabulary\"\n",
      "\"word 'antilopine' not in vocabulary\"\n",
      "\"word 'antilopinus' not in vocabulary\"\n",
      "\"word 'macropodidae' not in vocabulary\"\n",
      "\"word 'eructation' not in vocabulary\"\n",
      "\"word 'megalania' not in vocabulary\"\n",
      "\"word 'wonambi' not in vocabulary\"\n",
      "\"word 'disembowelling' not in vocabulary\"\n",
      "\"word 'hindleg' not in vocabulary\"\n",
      "\"word 'wallal' not in vocabulary\"\n",
      "\"word 'aherrenge' not in vocabulary\"\n",
      "\"word 'grook' not in vocabulary\"\n",
      "\"word 'kurnai' not in vocabulary\"\n",
      "\"word 'hindleg' not in vocabulary\"\n",
      "\"word '8014' not in vocabulary\"\n",
      "\"word '8262' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "for word in data[0]:\n",
    "    try:\n",
    "        vector=model[word]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        wvc.append(vector)\n",
    "        #print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
